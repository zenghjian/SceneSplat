#!/usr/bin/env python3
"""
SceneSplatæ¨ç†ç»“æœç®€åŒ–ä¿å­˜è„šæœ¬
åªä¿å­˜æ ¸å¿ƒå¿…è¦æ–‡ä»¶ï¼Œå‡å°‘æ–‡ä»¶æ•°é‡
"""
import os
import numpy as np
import torch
import pickle
from datetime import datetime

def save_inference_output_simple(output, input_dict, scene_name, results_dir="/home/huajianzeng/project/SceneSplat/results"):
    """
    ç®€åŒ–ç‰ˆæ¨ç†è¾“å‡ºä¿å­˜ - åªä¿å­˜æ ¸å¿ƒæ–‡ä»¶
    
    Args:
        output: æ¨¡å‹è¾“å‡º
        input_dict: è¾“å…¥æ•°æ®å­—å…¸
        scene_name: åœºæ™¯åç§°
        results_dir: ä¿å­˜ç›®å½•
    
    Returns:
        dict: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å­—å…¸
    """
    print(f"\nğŸ’¾ ç®€åŒ–ä¿å­˜æ¨ç†è¾“å‡º...")
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(results_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    saved_files = {}
    
    try:
        # 1. ä¿å­˜å®Œæ•´è¾“å‡ºç»“æ„ (pickleæ ¼å¼ï¼Œä¿æŒåŸå§‹ç»“æ„)
        output_file = os.path.join(results_dir, f"{scene_name}_{timestamp}_full_output.pkl")
        output_for_save = {}
        
        if isinstance(output, dict):
            for k, v in output.items():
                if isinstance(v, torch.Tensor):
                    output_for_save[k] = v.detach().cpu()
                elif hasattr(v, '__dict__'):
                    # å¤„ç†Pointå¯¹è±¡ç­‰è‡ªå®šä¹‰ç±»
                    obj_dict = {}
                    for attr_name, attr_value in v.__dict__.items():
                        if isinstance(attr_value, torch.Tensor):
                            obj_dict[attr_name] = attr_value.detach().cpu()
                        else:
                            obj_dict[attr_name] = attr_value
                    output_for_save[k] = obj_dict
                else:
                    output_for_save[k] = v
        else:
            if isinstance(output, torch.Tensor):
                output_for_save = output.detach().cpu()
            else:
                output_for_save = output
        
        with open(output_file, 'wb') as f:
            pickle.dump(output_for_save, f)
        
        saved_files['full_output'] = output_file
        print(f"   âœ… å®Œæ•´è¾“å‡º: {os.path.basename(output_file)}")
        
        # 2. ä¿å­˜ä¸»è¦è¾“å‡ºç‰¹å¾ (æœ€é‡è¦)
        if isinstance(output, dict) and 'point_feat' in output:
            point_feat = output['point_feat']
            
            if hasattr(point_feat, 'feat') and point_feat.feat is not None:
                # ä¸»è¦ç‰¹å¾ - è¿™æ˜¯æœ€é‡è¦çš„è¾“å‡º
                feat_array = point_feat.feat.detach().cpu().numpy()
                feat_file = os.path.join(results_dir, f"{scene_name}_{timestamp}_features.npy")
                np.save(feat_file, feat_array)
                saved_files['features'] = feat_file
                print(f"   âœ… ä¸»è¦ç‰¹å¾: {os.path.basename(feat_file)} {feat_array.shape}")
            
            if hasattr(point_feat, 'coord') and point_feat.coord is not None:
                # è¾“å‡ºåæ ‡ - ä¸ç‰¹å¾å¯¹åº”çš„3Dä½ç½®
                coord_array = point_feat.coord.detach().cpu().numpy()
                coord_file = os.path.join(results_dir, f"{scene_name}_{timestamp}_coords.npy")
                np.save(coord_file, coord_array)
                saved_files['coords'] = coord_file
                print(f"   âœ… è¾“å‡ºåæ ‡: {os.path.basename(coord_file)} {coord_array.shape}")
        
        elif isinstance(output, torch.Tensor):
            # å¦‚æœè¾“å‡ºæ˜¯å•ä¸ªtensor
            feat_array = output.detach().cpu().numpy()
            feat_file = os.path.join(results_dir, f"{scene_name}_{timestamp}_features.npy")
            np.save(feat_file, feat_array)
            saved_files['features'] = feat_file
            print(f"   âœ… è¾“å‡ºç‰¹å¾: {os.path.basename(feat_file)} {feat_array.shape}")
        
        # 2. ä¿å­˜è¾“å…¥æ•°æ®ç”¨äºå¯¹æ¯” (å¯é€‰ä½†æ¨è)
        if input_dict:
            # åŸå§‹è¯­è¨€ç‰¹å¾ - ç”¨äºå¯¹æ¯”åˆ†æ
            if 'lang_feat' in input_dict:
                lang_array = input_dict['lang_feat'].detach().cpu().numpy()
                lang_file = os.path.join(results_dir, f"{scene_name}_{timestamp}_input_lang.npy")
                np.save(lang_file, lang_array)
                saved_files['input_lang'] = lang_file
                print(f"   âœ… è¾“å…¥è¯­è¨€ç‰¹å¾: {os.path.basename(lang_file)} {lang_array.shape}")
            
            # åŸå§‹å‡ ä½•ç‰¹å¾ - ç”¨äºç†è§£è¾“å…¥
            if 'feat' in input_dict:
                geom_array = input_dict['feat'].detach().cpu().numpy()
                geom_file = os.path.join(results_dir, f"{scene_name}_{timestamp}_input_geom.npy")
                np.save(geom_file, geom_array)
                saved_files['input_geom'] = geom_file
                print(f"   âœ… è¾“å…¥å‡ ä½•ç‰¹å¾: {os.path.basename(geom_file)} {geom_array.shape}")
        
        # 3. ç”Ÿæˆå¿«é€ŸåŠ è½½è„šæœ¬
        load_script = os.path.join(results_dir, f"load_{scene_name}_{timestamp}.py")
        with open(load_script, 'w') as f:
            f.write(f'''#!/usr/bin/env python3
"""
å¿«é€ŸåŠ è½½SceneSplatæ¨ç†ç»“æœ - ç®€åŒ–ç‰ˆ
ç”Ÿæˆæ—¶é—´: {datetime.now()}
åœºæ™¯: {scene_name}
"""
import numpy as np
import pickle
import os

def load_results():
    """åŠ è½½æ ¸å¿ƒæ¨ç†ç»“æœ"""
    results = {{}}
    
    print("ğŸ”„ åŠ è½½æ¨ç†ç»“æœ...")
    
    # åŠ è½½å®Œæ•´è¾“å‡º
    full_output_file = "{output_file}"
    if os.path.exists(full_output_file):
        with open(full_output_file, 'rb') as f:
            results['full_output'] = pickle.load(f)
        print("âœ… å®Œæ•´è¾“å‡ºå·²åŠ è½½")
    
''')
            
            # æ·»åŠ åŠ è½½ä»£ç 
            for key, file_path in saved_files.items():
                if file_path.endswith('.npy'):
                    f.write(f'''    # {key}
    {key}_file = "{file_path}"
    if os.path.exists({key}_file):
        results['{key}'] = np.load({key}_file)
        print(f"âœ… {key}: {{results['{key}'].shape}}")
    else:
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {{{key}_file}}")
    
''')
            
            f.write('''    return results

def analyze_features():
    """åˆ†æä¸»è¦ç‰¹å¾"""
    data = load_results()
    
    if 'features' not in data:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä¸»è¦ç‰¹å¾")
        return
    
    features = data['features']
    print(f"\\nğŸ“Š ç‰¹å¾åˆ†æ:")
    print(f"   å½¢çŠ¶: {features.shape}")
    print(f"   ç±»å‹: {features.dtype}")
    print(f"   èŒƒå›´: [{features.min():.6f}, {features.max():.6f}]")
    print(f"   å‡å€¼: {features.mean():.6f}")
    print(f"   æ ‡å‡†å·®: {features.std():.6f}")
    
    # ä¸è¾“å…¥è¯­è¨€ç‰¹å¾å¯¹æ¯”
    if 'input_lang' in data:
        input_lang = data['input_lang']
        print(f"\\nğŸ” ä¸è¾“å…¥è¯­è¨€ç‰¹å¾å¯¹æ¯”:")
        print(f"   è¾“å…¥å½¢çŠ¶: {input_lang.shape}")
        print(f"   è¾“å…¥èŒƒå›´: [{input_lang.min():.6f}, {input_lang.max():.6f}]")
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        if features.shape == input_lang.shape:
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            features_norm = features / np.linalg.norm(features, axis=1, keepdims=True)
            input_norm = input_lang / np.linalg.norm(input_lang, axis=1, keepdims=True)
            cosine_sim = np.mean(np.sum(features_norm * input_norm, axis=1))
            print(f"   ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_sim:.6f}")
            
            if cosine_sim > 0.8:
                print("   ğŸ” é«˜ç›¸ä¼¼åº¦ - ä¸»è¦ä¿æŒäº†è¾“å…¥è¯­è¨€ç‰¹å¾")
            elif cosine_sim > 0.3:
                print("   ğŸ” ä¸­ç­‰ç›¸ä¼¼åº¦ - è¯­è¨€ä¸å‡ ä½•ç‰¹å¾èåˆ")
            else:
                print("   ğŸ” ä½ç›¸ä¼¼åº¦ - å¤§å¹…ç‰¹å¾å˜æ¢")
    
    return data

if __name__ == "__main__":
    print("SceneSplatæ¨ç†ç»“æœåŠ è½½å™¨ - ç®€åŒ–ç‰ˆ")
    print("=" * 50)
    
    # åŠ è½½å¹¶åˆ†æ
    data = analyze_features()
    
    print(f"\\nâœ… å®Œæˆ! å¯ç”¨æ•°æ®: {list(data.keys()) if data else 'æ— '}")
    
    print("\\nğŸ’¡ ä½¿ç”¨æç¤º:")
    print("   - ä¸»è¦è¾“å‡ºç‰¹å¾åœ¨ data['features'] ä¸­")
    print("   - å¯¹åº”åæ ‡åœ¨ data['coords'] ä¸­ (å¦‚æœæœ‰)")
    print("   - è¾“å…¥è¯­è¨€ç‰¹å¾åœ¨ data['input_lang'] ä¸­")
''')
        
        saved_files['load_script'] = load_script
        print(f"   âœ… å¿«é€ŸåŠ è½½è„šæœ¬: {os.path.basename(load_script)}")
        
        # 4. ç”Ÿæˆç®€è¦æ‘˜è¦
        summary_file = os.path.join(results_dir, f"{scene_name}_{timestamp}_summary.txt")
        with open(summary_file, 'w') as f:
            f.write(f"SceneSplatæ¨ç†ç»“æœ - ç®€åŒ–ç‰ˆ\n")
            f.write(f"=" * 40 + "\n\n")
            f.write(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n")
            f.write(f"åœºæ™¯: {scene_name}\\n\\n")
            
            f.write("ä¿å­˜æ–‡ä»¶:\\n")
            for key, file_path in saved_files.items():
                f.write(f"- {key}: {os.path.basename(file_path)}\\n")
            
            f.write(f"\\næ ¸å¿ƒæ–‡ä»¶è¯´æ˜:\\n")
            if 'full_output' in saved_files:
                f.write(f"- full_output.pkl: å®Œæ•´æ¨ç†è¾“å‡ºï¼Œä¿æŒåŸå§‹ç»“æ„\\n")
            if 'features' in saved_files:
                f.write(f"- features.npy: ä¸»è¦è¾“å‡ºç‰¹å¾ï¼Œç”¨äºä¸‹æ¸¸ä»»åŠ¡\\n")
            if 'coords' in saved_files:
                f.write(f"- coords.npy: ç‰¹å¾å¯¹åº”çš„3Dåæ ‡\\n")
            if 'input_lang' in saved_files:
                f.write(f"- input_lang.npy: è¾“å…¥è¯­è¨€ç‰¹å¾ï¼Œç”¨äºå¯¹æ¯”\\n")
            if 'input_geom' in saved_files:
                f.write(f"- input_geom.npy: è¾“å…¥å‡ ä½•ç‰¹å¾\\n")
            
            f.write(f"\\nä½¿ç”¨æ–¹æ³•:\\n")
            f.write(f"python {os.path.basename(load_script)}\\n")
        
        saved_files['summary'] = summary_file
        print(f"   âœ… æ‘˜è¦æ–‡ä»¶: {os.path.basename(summary_file)}")
        
        print(f"\\nğŸ“ æ–‡ä»¶ä¿å­˜åœ¨: {results_dir}")
        npy_count = len([f for f in saved_files.values() if f.endswith('.npy')])
        pkl_count = len([f for f in saved_files.values() if f.endswith('.pkl')])
        print(f"ğŸ¯ æ ¸å¿ƒæ–‡ä»¶: {npy_count} ä¸ªnpyæ–‡ä»¶ + {pkl_count} ä¸ªpklæ–‡ä»¶")
        print(f"ğŸ“‹ ä¸»è¦è¾“å‡º: {os.path.basename(saved_files.get('features', 'N/A'))}")
        print(f"ğŸ“¦ å®Œæ•´è¾“å‡º: {os.path.basename(saved_files.get('full_output', 'N/A'))}")
        
        return saved_files
        
    except Exception as e:
        print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {}

def load_simple_results(results_dir, scene_name=None, timestamp=None):
    """
    åŠ è½½ç®€åŒ–ä¿å­˜çš„ç»“æœ
    
    Args:
        results_dir: ç»“æœç›®å½•
        scene_name: åœºæ™¯åç§°ï¼ˆå¯é€‰ï¼‰
        timestamp: æ—¶é—´æˆ³ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        dict: æ ¸å¿ƒç‰¹å¾æ•°æ®
    """
    if not os.path.exists(results_dir):
        print(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {results_dir}")
        return {}
    
    files = os.listdir(results_dir)
    
    # æŸ¥æ‰¾åŒ¹é…æ–‡ä»¶
    if scene_name and timestamp:
        prefix = f"{scene_name}_{timestamp}_"
    elif scene_name:
        # æ‰¾åˆ°è¯¥åœºæ™¯æœ€æ–°çš„ç»“æœ
        scene_files = [f for f in files if f.startswith(f"{scene_name}_") and f.endswith('.npy')]
        if not scene_files:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°åœºæ™¯ {scene_name} çš„ç»“æœ")
            return {}
        # å–æœ€æ–°çš„æ—¶é—´æˆ³
        timestamps = list(set([f.split('_')[2] for f in scene_files if len(f.split('_')) >= 3]))
        timestamp = max(timestamps) if timestamps else None
        prefix = f"{scene_name}_{timestamp}_" if timestamp else f"{scene_name}_"
    else:
        print("âŒ è¯·æä¾›åœºæ™¯åç§°")
        return {}
    
    results = {}
    
    # åŠ è½½æ ¸å¿ƒæ–‡ä»¶
    core_files = {
        'features': f"{prefix}features.npy",
        'coords': f"{prefix}coords.npy", 
        'input_lang': f"{prefix}input_lang.npy",
        'input_geom': f"{prefix}input_geom.npy"
    }
    
    for key, filename in core_files.items():
        file_path = os.path.join(results_dir, filename)
        if os.path.exists(file_path):
            try:
                results[key] = np.load(file_path)
                print(f"âœ… {key}: {results[key].shape}")
            except Exception as e:
                print(f"âŒ åŠ è½½å¤±è´¥ {filename}: {e}")
    
    return results

if __name__ == "__main__":
    print("SceneSplatç®€åŒ–ç‰¹å¾ä¿å­˜å·¥å…·")
    print("åŠŸèƒ½: åªä¿å­˜æ ¸å¿ƒå¿…è¦æ–‡ä»¶ï¼Œå‡å°‘æ–‡ä»¶æ•°é‡")
    print("ä½¿ç”¨: save_inference_output_simple(output, input_dict, scene_name)")